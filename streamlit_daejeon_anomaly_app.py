
import io
import json
import math
from pathlib import Path
from typing import Tuple, Optional

import numpy as np
import pandas as pd
import streamlit as st

st.set_page_config(page_title="ë¶€ë™ì‚° ì´ìƒê±°ëž˜ íƒì§€", layout="wide")

# -----------------------------
# Utilities
# -----------------------------
def to_py(m2: float) -> float:
    return m2 * 0.3025  # 1ãŽ¡ â‰ˆ 0.3025í‰

def price_per_py(price: float, m2: float) -> float:
    py = to_py(m2) or 1.0
    return price / py

def stddev(arr: np.ndarray) -> float:
    if len(arr) < 2:
        return 0.0
    return float(np.std(arr, ddof=1))

def key_of(row: pd.Series) -> str:
    return f"{row['complex']}__{row['area']}__{row['date']}"  # ì›”Â·ë‹¨ì§€Â·ë©´ì 

@st.cache_data(show_spinner=False)
def read_json_records(path: Path) -> pd.DataFrame:
    data = json.loads(path.read_text(encoding="utf-8"))
    df = pd.DataFrame(data)
    # basic type fix
    num_cols = ["area","price","floor","buildYear"]
    for c in num_cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df

def _find_header_line(lines) -> Optional[int]:
    # Heuristics: look for a line that contains common headers including "ì‹œêµ°êµ¬" and "ë‹¨ì§€ëª…"
    for idx, line in enumerate(lines[:200]):
        if ("ì‹œêµ°êµ¬" in line) and ("ë‹¨ì§€ëª…" in line) and ("ê±°ëž˜ê¸ˆì•¡" in line):
            return idx
    # Fallback: known MOLIT export has header around line 15 (0-indexed)
    return 15

@st.cache_data(show_spinner=False)
def read_molit_csv(file_bytes: bytes) -> pd.DataFrame:
    # Try cp949 first (common for MOLIT export). Ignore errors.
    text = file_bytes.decode("cp949", errors="ignore")
    lines = text.splitlines()
    header_idx = _find_header_line(lines)
    # Rebuild clean CSV starting from header
    cleaned = "\n".join(lines[header_idx:])
    df = pd.read_csv(
        io.StringIO(cleaned),
        engine="python",
        quotechar='"',
        thousands=",",
    )
    need_cols = ["ì‹œêµ°êµ¬","ë‹¨ì§€ëª…","ì „ìš©ë©´ì (ãŽ¡)","ê³„ì•½ë…„ì›”","ê³„ì•½ì¼","ê±°ëž˜ê¸ˆì•¡(ë§Œì›)","ì¸µ","ê±´ì¶•ë…„ë„"]
    miss = [c for c in need_cols if c not in df.columns]
    if miss:
        raise ValueError(f"CSV í—¤ë”ì— ë‹¤ìŒ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {miss}")
    # Parse region
    def parse_region(s: str) -> Tuple[str,str]:
        parts = str(s).split()
        if len(parts) >= 3:
            return parts[1], parts[2]  # êµ¬, ë™
        if len(parts) == 2:
            return parts[1], ""
        return parts[0] if parts else "", ""

    district, dong = zip(*df["ì‹œêµ°êµ¬"].map(parse_region))
    out = pd.DataFrame({
        "district": district,
        "dong": dong,
        "complex": df["ë‹¨ì§€ëª…"].astype(str),
        "area": pd.to_numeric(df["ì „ìš©ë©´ì (ãŽ¡)"], errors="coerce"),
        "price": pd.to_numeric(df["ê±°ëž˜ê¸ˆì•¡(ë§Œì›)"], errors="coerce"),
        "date": df["ê³„ì•½ë…„ì›”"].apply(lambda v: f"{int(v)//100:04d}-{int(v)%100:02d}" if pd.notna(v) else ""),
        "floor": pd.to_numeric(df["ì¸µ"], errors="coerce"),
        "buildYear": pd.to_numeric(df["ê±´ì¶•ë…„ë„"], errors="coerce"),
    })
    out = out.dropna(subset=["area","price","floor","buildYear"]).reset_index(drop=True)
    out.insert(0, "id", np.arange(1, len(out)+1))
    return out

# -----------------------------
# Data loading
# -----------------------------
st.title("ðŸ™ï¸ ë¶€ë™ì‚° ì´ìƒê±°ëž˜ íƒì§€ ì‹œìŠ¤í…œ (Streamlit)")
st.caption("ë™ì¼ ì›”Â·ë‹¨ì§€Â·ë©´ì  ê¸°ì¤€ í†µê³„ + Z-score ê¸°ë°˜ ì •ëŸ‰ íƒì§€")

col_left, col_right = st.columns([2,1])
with col_right:
    st.markdown("#### ë°ì´í„° ì†ŒìŠ¤")
    default_json_path = Path("daejeon_trades_2025.json")
    use_uploaded = st.toggle("ì—…ë¡œë“œí•œ íŒŒì¼ ì‚¬ìš©(ê¶Œìž¥)", value=False, help="MOLIT CSV ë˜ëŠ” ìœ„ JSON ìŠ¤í‚¤ë§ˆ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì„œ ë¶„ì„í•©ë‹ˆë‹¤.")
    uploaded = st.file_uploader("CSV(JSON) ì—…ë¡œë“œ", type=["csv","json"], accept_multiple_files=False)

# Load logic
df: Optional[pd.DataFrame] = None
load_err: Optional[str] = None
if use_uploaded and uploaded is not None:
    try:
        if uploaded.name.lower().endswith(".json"):
            df = pd.DataFrame(json.loads(uploaded.getvalue().decode("utf-8")))
            num_cols = ["area","price","floor","buildYear"]
            for c in num_cols:
                if c in df.columns:
                    df[c] = pd.to_numeric(df[c], errors="coerce")
            if "id" not in df.columns:
                df.insert(0, "id", np.arange(1, len(df)+1))
        else:
            df = read_molit_csv(uploaded.getvalue())
    except Exception as e:
        load_err = f"ì—…ë¡œë“œ íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {e}"
else:
    try:
        if default_json_path.exists():
            df = read_json_records(default_json_path)
            if "id" not in df.columns:
                df.insert(0, "id", np.arange(1, len(df)+1))
        else:
            load_err = "í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— 'daejeon_trades_2025.json' íŒŒì¼ì„ ë‘ê±°ë‚˜, CSV/JSONì„ ì—…ë¡œë“œí•˜ì„¸ìš”."
    except Exception as e:
        load_err = f"ê¸°ë³¸ JSON ì½ê¸° ì‹¤íŒ¨: {e}"

if load_err:
    st.error(load_err)
    st.stop()

if df is None or df.empty:
    st.warning("ë°ì´í„°ê°€ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤.")
    st.stop()

# Ensure column types
df["district"] = df["district"].astype(str)
df["dong"] = df["dong"].astype(str)
df["complex"] = df["complex"].astype(str)
df["date"] = df["date"].astype(str)

# -----------------------------
# Sidebar controls
# -----------------------------
st.sidebar.header("âš™ï¸ í•„í„° & ì„¤ì •")
months = ["ì „ì²´"] + sorted([m for m in df["date"].dropna().unique().tolist() if m])
districts = ["ì „ì²´"] + sorted(df["district"].dropna().unique().tolist())

selected_district = st.sidebar.selectbox("ìžì¹˜êµ¬", districts, index=0)
selected_month = st.sidebar.selectbox("ì›” ì„ íƒ", months, index=0)
min_score = st.sidebar.slider("ë¯¼ê°ë„ (ì´ìƒ ì‹ í˜¸ ì ìˆ˜ â‰¥)", min_value=10, max_value=70, value=25, step=5)
use_z = st.sidebar.checkbox("Z-score ê·œì¹™ ì‚¬ìš© (Â±2Ïƒ)", value=True)
sort_opt = st.sidebar.selectbox("ì •ë ¬", ["ìœ„í—˜ë„ ë†’ì€ ìˆœ","ê°€ê²© ë†’ì€ ìˆœ","ê°€ê²© ë‚®ì€ ìˆœ"], index=0)

# -----------------------------
# Group statistics (ì›”Â·ë‹¨ì§€Â·ë©´ì )
# -----------------------------
mask = (selected_district == "ì „ì²´") | (df["district"] == selected_district)
mask_month = (selected_month == "ì „ì²´") | (df["date"] == selected_month)
dfv = df[mask & mask_month].copy()

# Build group stats
group_key = dfv.apply(lambda r: key_of(r), axis=1)
dfv["_gk"] = group_key
gp = dfv.groupby("_gk")["price"].agg(["mean","std","count"]).rename(columns={"mean":"avg","std":"std","count":"n"})
dfv = dfv.join(gp, on="_gk")

# Compute anomaly metrics
dfv["deviationPct"] = (dfv["price"] - dfv["avg"]) / dfv["avg"].replace(0, np.nan) * 100.0
dfv["py"] = (dfv["price"] / (dfv["area"] * 0.3025)).round()  # price per py
dfv["avgPy"] = (dfv["avg"] / (dfv["area"] * 0.3025)).round()
dfv["pyDeviationAbs"] = (dfv["py"] - dfv["avgPy"]).abs()
dfv["z"] = (dfv["price"] - dfv["avg"]) / dfv["std"].replace(0, np.nan)
dfv["z"] = dfv["z"].fillna(0.0)

# Scoring
score = np.zeros(len(dfv), dtype=float)
reasons = [[] for _ in range(len(dfv))]

up_mask = dfv["deviationPct"] > 30
down_mask = dfv["deviationPct"] < -20
low_floor_up = (dfv["floor"] <= 5) & (dfv["deviationPct"] > 10)
py_anom = dfv["pyDeviationAbs"] > (dfv["avgPy"] * 0.25)
z_anom = use_z & (dfv["z"].abs() >= 2)

score = score + np.where(up_mask, 40, 0)
score = score + np.where(down_mask, 35, 0)
score = score + np.where(low_floor_up, 15, 0)
score = score + np.where(py_anom, 20, 0)
score = score + np.where(z_anom, 25, 0)

def push_reason(i, cond, text):
    if cond.iloc[i]:
        reasons[i].append(text)

for i in range(len(dfv)):
    push_reason(i, up_mask, "ê¸‰ê²©í•œ ê°€ê²© ìƒìŠ¹")
    push_reason(i, down_mask, "ê¸‰ê²©í•œ ê°€ê²© í•˜ë½")
    push_reason(i, low_floor_up, "ì €ì¸µ ê°€ê²© ì´ìƒ")
    push_reason(i, py_anom, "í‰ë‹¹ê°€ ì´ìƒ")
    if use_z and abs(float(dfv.iloc[i]['z'])) >= 2:
        reasons[i].append(f"Z-score {dfv.iloc[i]['z']:.1f}Ïƒ ì´ìƒ")

dfv["anomalyScore"] = np.minimum(100, np.round(score)).astype(int)
dfv["anomalyReasons"] = reasons
dfv["isAnomaly"] = dfv["anomalyScore"] >= min_score

# Sorting
if sort_opt == "ê°€ê²© ë†’ì€ ìˆœ":
    dfv = dfv.sort_values(by=["price","anomalyScore"], ascending=[False, False])
elif sort_opt == "ê°€ê²© ë‚®ì€ ìˆœ":
    dfv = dfv.sort_values(by=["price","anomalyScore"], ascending=[True, False])
else:
    dfv = dfv.sort_values(by=["isAnomaly","anomalyScore","price"], ascending=[False, False, False])

# Stats
anoms = dfv[dfv["isAnomaly"]]
high = int((anoms["deviationPct"] > 0).sum())
low = int((anoms["deviationPct"] < 0).sum())
rate = (len(anoms) / len(dfv) * 100.0) if len(dfv) else 0.0

# -----------------------------
# Top summary cards
# -----------------------------
c1, c2, c3, c4 = st.columns(4)
c1.metric("ë¶„ì„ ëŒ€ìƒ", f"{len(dfv):,} ê±´")
c2.metric("ì´ìƒê±°ëž˜", f"{len(anoms):,} ê±´", f"{rate:.1f}%")
c3.metric("ê¸‰ë“± ì˜ì‹¬", f"{high:,} ê±´")
c4.metric("ê¸‰ë½ ì˜ì‹¬", f"{low:,} ê±´")

st.divider()

# -----------------------------
# Results Table
# -----------------------------
st.subheader(f"ì´ìƒê±°ëž˜ íƒì§€ ê²°ê³¼ (í•„í„° ì ìš© í›„ {len(anoms):,}ê±´)")
show_only_anom = st.toggle("ì´ìƒê±°ëž˜ë§Œ ë³´ê¸°", value=True)

columns_order = [
    "anomalyScore","district","complex","dong","area","price","py",
    "avg","avgPy","deviationPct","z","date","floor","buildYear","anomalyReasons"
]
df_show = dfv.copy()
if show_only_anom:
    df_show = df_show[df_show["isAnomaly"]]

def fmt_reason(xs):
    return " / ".join(xs) if isinstance(xs, list) else ""

out = df_show[columns_order].copy()
out = out.rename(columns={
    "anomalyScore": "ìœ„í—˜ë„",
    "district": "ìžì¹˜êµ¬",
    "complex": "ë‹¨ì§€ëª…",
    "dong": "ë™",
    "area": "ë©´ì (ãŽ¡)",
    "price": "ê±°ëž˜ê°€(ë§Œì›)",
    "py": "í‰ë‹¹ê°€(ë§Œì›)",
    "avg": "í‰ê· ê°€(ë§Œì›)",
    "avgPy": "í‰ê·  í‰ë‹¹ê°€(ë§Œì›)",
    "deviationPct": "íŽ¸ì°¨(%)",
    "z": "Z",
    "date": "ì›”",
    "floor": "ì¸µ",
    "buildYear": "ê±´ì¶•ë…„ë„",
    "anomalyReasons": "ì´ìƒì‚¬ìœ ",
})
out["íŽ¸ì°¨(%)"] = out["íŽ¸ì°¨(%)"].map(lambda v: f"{v:+.1f}%" if pd.notna(v) else "")
out["í‰ë‹¹ê°€(ë§Œì›)"] = out["í‰ë‹¹ê°€(ë§Œì›)"].map(lambda v: f"{int(v):,}")
out["í‰ê·  í‰ë‹¹ê°€(ë§Œì›)"] = out["í‰ê·  í‰ë‹¹ê°€(ë§Œì›)"].map(lambda v: f"{int(v):,}")
out["ê±°ëž˜ê°€(ë§Œì›)"] = out["ê±°ëž˜ê°€(ë§Œì›)"].map(lambda v: f"{int(v):,}")
out["í‰ê· ê°€(ë§Œì›)"] = out["í‰ê· ê°€(ë§Œì›)"].map(lambda v: f"{int(v):,}")
out["ì´ìƒì‚¬ìœ "] = df_show["anomalyReasons"].map(fmt_reason)

st.dataframe(out, use_container_width=True, height=600)

# -----------------------------
# Download
# -----------------------------
st.download_button(
    "í˜„ìž¬ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (CSV)",
    data=df_show.to_csv(index=False).encode("utf-8-sig"),
    file_name="anomaly_results.csv",
    mime="text/csv",
)
st.caption("â€» ê²°ê³¼ CSVëŠ” í•„í„°/ë¯¼ê°ë„/Z-score ì„¤ì •ì´ ë°˜ì˜ëœ í˜„ìž¬ í™”ë©´ ê¸°ì¤€ìž…ë‹ˆë‹¤.")

# -----------------------------
# Help
# -----------------------------
with st.expander("ðŸ“˜ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ì„¤ëª…", expanded=False):
    st.markdown(
        """
- **ì§‘ê³„ ê¸°ì¤€**: ë™ì¼ **ì›”Â·ë‹¨ì§€Â·ë©´ì ** ê·¸ë£¹ í‰ê·  ë° í‘œì¤€íŽ¸ì°¨ ì‚¬ìš©
- **ê¸‰ê²©í•œ ê°€ê²© ìƒìŠ¹**: ê·¸ë£¹ í‰ê·  ëŒ€ë¹„ **+30%** ì´ìƒ
- **ê¸‰ê²©í•œ ê°€ê²© í•˜ë½**: ê·¸ë£¹ í‰ê·  ëŒ€ë¹„ **âˆ’20%** ì´í•˜
- **ì €ì¸µ ê°€ê²© ì´ìƒ**: **5ì¸µ ì´í•˜**ì´ë©´ì„œ **+10%** ì´ˆê³¼
- **í‰ë‹¹ê°€ ì´ìƒ**: ê·¸ë£¹ í‰ê·  í‰ë‹¹ê°€ ëŒ€ë¹„ **25%** ì´ˆê³¼
- **Z-score**: \\(|Z| â‰¥ 2\\) ì‹œ **+25ì ** ê°€ì (ì˜µì…˜)
        """
    )
